
== Lessons Learned

Participants of the various organizations and institutes that contribute to the Climate Resilience Pilot noted the following gaps or challenges do still exist and require additional work (in future) to overcome:

Currently, the Pixalytics Drought indicator utilizes data from the Copernicus Climate Data Store (CDS). However, the project is in the process of testing various other sources and datasets to assess the speed, reliability, and cost of accessing input data from different providers. The goal is to enable on-demand data processing.

During the testing phase, we compared the input precipitation data obtained from the ERA5 dataset within the Registry of Open Data on AWS to the CDS API. It was found that accessing the data stored on Amazon Web Service (AWS) Simple Storage Service (S3) was faster once virtual Zarrs were set up. However, there are concerns regarding the data's provenance, as it was uploaded to AWS by an organization other than the original data provider. Additionally, the Zarr approach faced challenges when dealing with more recent years' data, as the NetCDFs stored on S3 had inconsistent chunking. To address this issue, a request has been submitted to enhance the Python kerchunk library's ability to handle variable chunking. We are pointing this out as it is not specific to this datasource; these challenges can happen to any large datasource that needs to transform into Zarrs to operate faster.

For ESRI's contribution, a few things learned in building CMRA version 1 and the last 6 months since its release:

•	Survey responses and conversations with users of the CRMA app and the underlying data have confirmed that providing open and usable data is equally important to providing a well designed web application. Both are needed, for different audiences.

•	There will always be groups that want their own application with their branding, even if its only slightly different than original.

•	Consider meaningful units of geography to summarize by based on the intended users/end-use. The polygons needs to be suitable size for the decisions being made, but not so small as to challenge the spatial resolution of the climate projection data.

•	When processing climate data into means, it’s critical to capture the extremes in space, time, and members. For version 2 we will compute the min, max, and mean of each variable and scenario combination.

•	It is important to communicate what variables are useful for what applications/considerations. E.g., why is 40 deg C a critical threshold? We are building this to expand access to a potentially less scientific and less climate aware audience and more explanation will be useful.

•	It is important to have a holistic integrated approach to data, tools, workflow documentation, and communities of practice. 

•	As we expand to a global system, issues of local downscaling and local weighting of models comes into question. Downscaled models do not exist in most of the world at a fine enough resolution to responsibly answer the type of questions users want to ask.

For IFC, the following lessons learned are highlighted:

•	Definition of common documentation guidelines for terms and licences of endpoints would facilitate approval in cybersecurity-heavy environments. Individual security reviews for each participants and their endpoints is a tedious process. Having common terms as defined in OGC’s master agreement, alongside applicable open source licences, would facilitate endpoint whitelisting.

•	In insurance companies, catastrophe (CAT) modeling is often looking at past events to establish probabilistic indices and maps [11]. Increasingly, climate projections are making their way into the CAT models. There is an opportunity for the OGC to advance standards in how these models are conceived, packaged, distributed and executed.

The pilot project aims to achieve multiple objectives, one of which is to reduce the obstacles that users face when accessing CDS/ADS (Atmospheric Data Store) data and services. By identifying these barriers or gaps from the users' perspective, the pilot can adapt and evolve accordingly. This approach ensures that the project engages a broader user community and facilitates their interaction with CDS/ADS.

To provide a clear direction for developers and users, the pilot intends to establish a universal and well-defined climate service workflow. This workflow will serve as a roadmap, guiding individuals through the entire process from raw data to actionable information. By offering a structured framework, the project aims to enhance efficiency and streamline the utilization of climate services.

Several enhancements were planned for the project, including improvements to the performance of the Sentinel-2 data cube. Climate data can be incorporated in this, and vegetation fuel type classification as well. This to support a wildfire risk assessment workflow. These enhancements contribute to expanding the capabilities and functionalities of the pilot project.

In regards to Analysis ready data, ARD principles can be applied to climate time series, not just earth observation (EO). Good ARD should be useful for a range of scenarios and useful to answer a range of analytic questions. ARD usually involves some degree of filtering, simplification and data aggregation without losing the essential information necessary to support decision making. 

During the DP21 phase, a solid foundation was established for exploring data cube extraction and conversion to ARD using the FME data integration platform. In this pilot, a number of new approaches were explored for tasks such as data extraction, simplification, and transformation. Additionally, different methods were investigated for selecting, splitting, aggregating, and summarizing time series. The primary objective was to generate ARD capable of answering questions related to climate trends and readily consumable by GIS and other geospatial applications.

The initial ARD approach to derive temperature and precipitation contours or polygons inherited from the work in DP21 on flood contours involved too much data simplification to be useful. Classification into temperature or participation bands resulted in an effective loss of detail, oversimplifying the data to the point where it no longer held enough variation over local areas to be useful. In discussion with other participants, it was determined that converting multidimensional data cubes to vector time series point data served the purpose of simplifying the data structure for ease of access, but retained the environmental variable precision needed to support a wider range of data interpretations for indicator derivation. It also meant that as a data provider we did not need to anticipate or encode interpretation of indicator business rules into our data simplification process. The end user is free to run queries to find locations and time steps for specific temperatures or precipitation ranges of interest.

Initially it was thought that classification rules need to more closely model impacts of interest. For example, the business rules for a heat wave might use a temperature range and stat type as part of the classification process before conversion to vector. However, this imposes the burden of domain knowledge on the data provider rather than on the climate service end user who is much more likely to understand the domain they wish to apply the data to and how best to interpret it.

In the absence of more sophisticated models, looking at the delta between future forecast and historical averages served as an interesting experiment for highlighting potential climate change impact hotspots. Past and future were differenced both spatially and temporally for equivalent time steps (monthly). These deltas may serve as a useful starting point for climate change risk indicator development. They also can serve as an approach for normalizing climate impacts when the absolute units are not the main focus. This may give local planners and managers more options to explore and analyze local areas and times of concern related to climate model scenario outputs.

More analysis needs to be done with higher resolution time steps - weekly and daily. At the outset monthly time steps were used to make it easier to prototype workflows. Daily time step computations will take significantly more processing time. Future pilots should explore ways of better supporting scalability of processing through automation and cloud computing approaches such as the use of cloud native formats (STAC, COG, ZARR etc).

Environmental climate variables (ECVs) have traditionally been discussed in the context of earth observation (EO) data. For the purposes of this and other OGC pilots, it also seems that ECVs could just as easily relate to the environmental variables stored in climate model outputs such as data cubes. Both store ECVs, its just that traditional EO ECVs relate to past or present observations while climate model ECVs relate to future potential or forecast ECV values. Either way, having a standardized understanding of what is meant by ECVs may go some way towards developing a better understanding of ARD in relation to climate change impact management.

Further experimentation is required to enhance the project's capabilities. This experimentation encompasses various aspects, including analytic techniques, statistical methods, simplification processes, and publication methodologies. Additionally, the project aims to explore cloud-native approaches such as NetCDF to COG conversion and the utilization of APIs. These ongoing experiments contribute to refining the project's methodologies and expanding its range of applications.

Currently, the participants have implemented the first Drought Index (SPI) using precipitation data from the Copernicus Climate Data Store (CDS). However, they are open to incorporating additional data sources as per the project's requirements. This flexibility ensures that the pilot project remains adaptable to evolving needs and can utilize diverse datasets to enhance its outputs.

In summary, the pilot project seeks to overcome barriers and engage a wider user community by facilitating access to CDS/ADS data and services. A well-defined climate service workflow will guide developers and users through the entire process, ensuring efficiency and effectiveness. Enhancements to the Sentinel-2 data cube, the inclusion of climate data and vegetation fuel type classification, and the development of a wildfire risk assessment workflow will expand the project's capabilities. By applying ARD principles and refining classification rules, the project aims to generate valuable insights into climate trends. Ongoing experimentation and the exploration of different methods contribute to the project's continuous improvement.



