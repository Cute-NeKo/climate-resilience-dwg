
//[[clause-reference]]
== Raw data to Analysis Ready Data (ARD)

CEOS defines Analysis Ready Data as satellite data that have been processed to a minimum set of requirements and organized into a form that allows immediate analysis with a minimum of additional user effort and interoperability both through time and with other datasets. See https://ceos.org/ard/, and especially the infornation for data producers: https://ceos.org/ard/files/CARD4L_Info_Note_Producers_v1.0.pdf.



//[[CRIS]]
//.CRIS overview
//image::CRIS.png[CRIS]

=== Jakub P. Walawender

- Component: Solar climate atlas for Poland.

- Inputs: In situ solar radiation and sunshine duration data, satellite-based solar radiation and sunshine duration estimates (climate data records), various different geospoatial data from different sources (e.g. digital elevation model, climate zones, etc.).

- Outputs:
  * This pilot outputs: Review of available solar radiation datasets and web services, 2 scripts (solar climate data exploratory analysis tool, solar climate data preprocessing tool), report summarizing results of the exploratory data analysis and quality control including discussion of inconsistency factors.
  * In the final result: solar radiation data cube for Poland (40 years of high resolution dataset for selected solar radiation variables), and analysis ready data (dedicated products for different solar-smart applications in the fields of renewable energy, agriculture, spatial planning, tourism, etc.), detailed analysis of the solar climate in Poland (incl. solar regionalisation) and online web map service with an interactive, self-explainable interface enabling easy on-demand information access.

- What other component(s) can interact with the component: This component work (considering the final result) crosses all the components and all of them are actually important.

- What OGC standards or formats does the component use and produce: 
  * NetCDF compliant with the CF (Climate and Forecast) convention. 
  * WMS, WCS, OGC API
  

=== Ecere Corporation

Ecere is providing a deployment of its GNOSIS Map Server with a focus on a Sentinel-2 Level 2A data cube. _OGC API - Tiles_, _OGC API - Coverages_, _OGC API - Maps_, _OGC API - Discrete Global Grid Systems_, _Common Query Language (CQL2)_, and  _OGC API - Processes - Part 3: Workflows & Chaining_ are the supported standards and extensions for this task.

The plan is to use machine learning process output from the Wildland Fire Fuel Indicator Workflow to identify vegetation fuel types from sentinel-2 bands, then combine with weather data to assess wildfire hazards risk in Australia.
The workflow will use as input the sentinel-2 OGC API data cube from our GNOSIS Map Server.

- Component: Data Cube and Wildfire vegetation fuel map / risk analysis.

- Inputs: ESA Sentinel-2 L2A data (from AWS / Element 84), Temperature / Precipitation / Wind climate data, Reference data for training: vegetation fuel type classification, wildfire risk.

The sentinel-2 Level 2A collection is provided at `https://maps.gnosis.earth/ogcapi/collections/sentinel2-l2a`

- Outputs: OGC API (Coverage, Tiles, DGGS, Maps) for Sentinel-2 data (https://maps.gnosis.earth/ogcapi/collections/sentinel2-l2a) including full global coverage, all resolutions/scales, all bands that can be individually selected, CQL2 expressions for band arithmetics; climate data (to be added), vegetation fuel type (possibly by end of pilot, or for DP2023), wildfire risk workflow (possibly by end of pilot, or for DP2023).

- What other component(s) can interact with the component: Any OGC API client component requiring efficient access to Sentinel-2 data, clients requiring climate data once made available, clients presenting vegetation fuel type, wildfire risk (once ready, might extend into DP2023).

- What OGC standards or formats does the component use and produce: 
  * OGC API (Coverage - with subsetting, scaling, range subsetting, coverage tiles; Tiles, DGGS (GNOSISGlobalGrid and ISEA9R), Maps (incl. map tiles), Styles), CQL2, OGC API - Processes with Part 3 for workflows (Nested Local/Remote Processes, Local/Remote Collection Input, Collection Output, Input/Output Field Modifiers) 
  * Formats: GNOSIS Map Tiles (Gridded Coverage, Vector Features, Map imagery, and more); GeoTIFF; PNG (16-bit value single channel for coverage, RGBA for maps); JPEG. 


==== Overview of standards and extensions available for outputs

===== OGC API - DGGS

There are two main requirements classes for this standard.

- Data Retrieval (What is here? -- "give me the data for this zone"),
- Zones Query (Where is it? -- "which zones match this collection and/or my query")

Example of data retrieval queries:

    https://maps.gnosis.earth/ogcapi/collections/sentinel2-l2a/dggs/GNOSISGlobalGrid/zones/3-4-11/data
    https://maps.gnosis.earth/ogcapi/collections/sentinel2-l2a/dggs/ISEA9Diamonds/zones/E7-FAE/data

Example of a zones query:

    https://maps.gnosis.earth/ogcapi/collections/SRTM_ViewFinderPanorama/dggs/ISEA9Diamonds/zones
    https://maps.gnosis.earth/ogcapi/collections/SRTM_ViewFinderPanorama/dggs/ISEA9Diamonds/zones?f=json (as a list of compact JSON IDs)

Level, Row, Column (which encoded differently in the compact hexadecimal zone IDs) can be seen on the zone information page at:

    https://maps.gnosis.earth/ogcapi/collections/sentinel2-l2a/dggs/GNOSISGlobalGrid/zones/3-4-11
    https://maps.gnosis.earth/ogcapi/collections/sentinel2-l2a/dggs/ISEA9Diamonds/zones/E7-FAE

There are several different discrete global grids. Two are implemented in our service:

- Our GNOSIS Global Grid, which is geographic rather than projected, and is axis-aligned with latitudes and longitudes, but not equal area (though it tends towards equal area -- maximum variation is ~48% up to a very detailed level)
- ISEA9R, which is a dual DGGS of ISEA3H even levels, using rhombuses/diamonds instead of hexagons, but much simpler to work with and can transport the hexagon area values as points on the rhombus vertices for those ISEA3H even levels. It is also axis-aligned to a CRS defined by rotating and skewing the ISEA projection.

The primary advantage of OGC API - DGGS is:

- for retrieving data from DGGS that are not axis-aligned or have geometry that cannot be represented as squares (e.g., hexagons), or
- for the zone query capability, most useful for specifying queries (e.g. using CQL2). The extent to which we implement Zones Query at this moment is still limited.

Examples of DGGS Zone information page:

[#ecere_dggs1,reftext='{figure-caption} {counter:figure-num}']
.GNOSIS Map Server information resource for GNOSIS Global Grid zone 5-24-6E
image::ecere_dggs1.PNG[]

[#ecere_dggs2,reftext='{figure-caption} {counter:figure-num}']
.GNOSIS Map Server information resource for ISEA9Diamonds zone 5-24-6E
image::ecere_dggs2.PNG[]

[#ecere_dggs3,reftext='{figure-caption} {counter:figure-num}']
.GNOSIS Map Server information resource for ISEA9Diamonds zone 5-24-6E sections
image::ecere_dggs3.PNG[]

===== OGC API - Coverages with OGC API - Tiles

Because they are axis-aligned, both of these DGGS can be described as a TileMatrixSet, and therefore equivalent functionality to the OGC API - DGGS Data Retrieval requirements class can be achieved using OGC API - Tiles and the corresponding TileMatrixSets instead.

Coverage Tile queries for the same zones:

    https://maps.gnosis.earth/ogcapi/collections/sentinel2-l2a/coverage/tiles/GNOSISGlobalGrid/3/4/17
    https://maps.gnosis.earth/ogcapi/collections/sentinel2-l2a/coverage/tiles/ISEA9Diamonds/4/373/288

To request a different band than the default RGB (B04, B03, B02) bands:

    https://maps.gnosis.earth/ogcapi/collections/sentinel2-l2a/coverage/tiles/GNOSISGlobalGrid/3/4/17?properties=B08
    https://maps.gnosis.earth/ogcapi/collections/sentinel2-l2a/coverage/tiles/ISEA9Diamonds/4/373/288?properties=B08

To retrieve coverage tiles with band arithmetic to compute NDVI:

    https://maps.gnosis.earth/ogcapi/collections/sentinel2-l2a/coverage/tiles/GNOSISGlobalGrid/3/4/17?properties=(B08/10000-B04/10000)/(B08/10000+B04/10000)
    https://maps.gnosis.earth/ogcapi/collections/sentinel2-l2a/coverage/tiles/ISEA9Diamonds/4/373/288?properties=(B08/10000-B04/10000)/(B08/10000+B04/10000)

===== OGC API - Maps with OGC API - Tiles

Map Tiles queries for the same zones:

    https://maps.gnosis.earth/ogcapi/collections/sentinel2-l2a/map/tiles/GNOSISGlobalGrid/3/4/17
    https://maps.gnosis.earth/ogcapi/collections/sentinel2-l2a/map/tiles/ISEA9Diamonds/4/373/288

[#ecere_mapggg,reftext='{figure-caption} {counter:figure-num}']
.GNOSIS Map Server Map of tiles 3/4/17 in GNOSISGlobalGrid
image::ecere_mapggg.PNG[]

To retrieve a map of the Scene Classification:

    https://maps.gnosis.earth/ogcapi/collections/sentinel2-l2a/styles/scl/map/tiles/GNOSISGlobalGrid/3/4/17
    https://maps.gnosis.earth/ogcapi/collections/sentinel2-l2a/styles/scl/map/tiles/ISEA9Diamonds/4/373/288

[#ecere_class,reftext='{figure-caption} {counter:figure-num}']
.Sentinel-2 with image classification styling
image::ecere_class.PNG[]

To filter out the clouds:

    https://maps.gnosis.earth/ogcapi/collections/sentinel2-l2a/map/tiles/GNOSISGlobalGrid/3/4/17?filter=SCL<8 or SCL >10
    https://maps.gnosis.earth/ogcapi/collections/sentinel2-l2a/map/tiles/ISEA9Diamonds/4/373/288?filter=SCL<8 or SCL >10

To get an NDVI map:

    https://maps.gnosis.earth/ogcapi/collections/sentinel2-l2a/styles/ndvi/map/tiles/GNOSISGlobalGrid/3/4/17
    https://maps.gnosis.earth/ogcapi/collections/sentinel2-l2a/styles/ndvi/map/tiles/ISEA9Diamonds/4/373/288

[#ecere_ndvi,reftext='{figure-caption} {counter:figure-num}']
.Sentinel-2 map with NDVI band arithmetic
image::ecere_ndvi.PNG[]

The same filter= and properties= should also work with the /coverage and /dggs end-points.
The filter= also works with the /map end-points.


=== Wuhan University (WHU)
Wuhan University (WHU) is a university that plays a significant role in researching and teaching all aspects of surveying and mapping, remote sensing, photogrammetry, and geospatial information sciences in China. In this Climate Resilience Pilot, we will contribute two use-cases: a use-case for drought and wildfire impact, and a use-case for analysis ready data.

- Component:  Data Cube and Drought Indicator.

- Inputs: Climate data, including precipitation and temperature. Optical data, such as Landsat-8 and sentinel-2.

- Outputs: Drought risk map and other results in the form of GeoTIFF after processing in a Data Cube.

- What other component(s) can interact with the component: .

- What OGC standards or formats does the component use and produce: 
  * OGC API - Coverages to provide the data in Cube
  * OGC API - Processes to provide the calculation of drought indices


=== Pixalytics

Pixalytics are developing an OGC-compliant Application Programming Interface (API) service, see <<Pixalytics_architecture>>, which will provide global information on droughts. The approach is to take global open data/datasets from organizations such as ESA/Copernicus, NASA/NOAA, and the WMO and combine meteorology, hydrology, and remote sensing data to produce ARD data based on a composite of different indicators. Where globally calculated drought indicators already exist, these are being used in preference to their re-calculation, although consistency and the presence of uncertainties are also being considered.

[[Pixalytics_architecture]]
.Pixalytics architecture
image::Pixalytics-architecture.png[Pixalytics-architecture]

Currently, the Standardized Precipitation Index (SPI) and Soil Moisture Anomaly (SMA) are being calculated using ERA5 reanalysis data from the Climate Data Store (CDS) of the Copernicus Climate Change Service. The API access is being set up following the Building Blocks for Climate Services (https://climateintelligence.github.io/smartduck-docs/) approach. Individual indices for precipitation (SPI), soil moisture (SMA), and vegetation drought are being worked on. Then, we will aim to combine them into a single view/indicator.

- Component: Drought indicator.

- Inputs: Meteorological data, including Precipitation, plus Land Surface Temperature, Soil Moisture, and Vegetation Index (or optical data to calculate it from).

- Outputs: Drought Indices - as a time-series dataset output in a choice of download formats: CSV, GeoJSON, CoverageJSON and NetCDF for point data and then COG for areas (to be developed).

- What other component(s) can interact with the component: a desire to link to visualization/DRI analysis components. A QGIS plugin has been updated to be able to perform a request and view the outputted JSON file (https://github.com/pixalytics-ltd/qgis-wps-plugin), and the Web Processing Service (WPS) link is https://api.pixalytics.com/climate/wps?request=GetCapabilities&service=wps

The WPS service remains in development/under improvement and currently provides access to individual precipitation and soil moisture related indices. Example Python query for a location in Canada (Latitude: 55.5 N Longitude: 99.1 W) for the SPI time series, with data for these dates/this location already cached, so runs quicker:

.Drought indicator calling code
----
    from owslib.wps import WebProcessingService, monitorExecution
    
	# contact the WPS client
    wps = WebProcessingService("http://api.pixalytics.com/climate/wps", skip_caps=True, verbose=False)
    
    # GetCapabilities
    wps.getcapabilities()

	# Execute
    inputs = [ ("start_date", '20200101'),
            ("end_date", '20221231'),
            ("latitude", '55.5'),
            ("longitude", '-99.1')]
    
    execution = wps.execute("drought", inputs, "output")

    monitorExecution(execution,download=True,filepath="temp.json")

    # Wait 5 seconds and check
    execution.checkStatus(sleepSecs=5)

    # show status
    print('Percent complete: {}'.format( execution.percentCompleted))

	# If there's an error print the error information
    for error in execution.errors:
        print("Error: ",error.code, error.locator, error.text)
----

- What OGC standards or formats does the component use and produce: Producing data on-the-fly using the WPS, so need to pull data through preferably an API route. The speed that the input data can be made available (i.e., extracting time-series subsets) governs the speed that the drought indicator provides data. To speed this up, input data that is not changing is being cached so that it runs significantly quicker when the API is called for a second time. 

<<Pixalytics_output>> shows an example of the output visualized within Python using Streamlit with the intermediate data (cached as NetCDF files) as input.

// This could be moved to a use case
[[Pixalytics_output]]
.Pixalytics output (Not the correct figure, to be updated)
image::Pixalytics-output-example.png[Pixalytics-output]



=== Safe Software

- Component:
 * Climate ARD component - Data Cube to ARD.
 * Impact Components general I/O (Heat, Drought, Flood).

- Inputs: 
 * Climate ARD component - Data Cube to ARD: Climate scenario data from climate services (NetCDF), for historic and future time periods
 * Impact Components general I/O (Heat, Drought, Flood): Climate impact ARD from Safes ARD component, including EO data (MODIS, LANDSAT, SENTINEL products), Population/Infrastructure information (OSM), Basemaps, as well as specific requirements per impact:
  * Drought: vegetation, soils, hydrology, basins
  * Flood: DEM, hydrology, basins.

- Outputs:
 * Climate ARD component - Data Cube to ARD: Gridded data, including temperature, soil moisture and  precipitation - aggregate grids (GeoTIFF/COG), as well as Vector data, including temperature, soil moisture and  precipitation contours (Geopackage, GeoJSON, OGC API Features).
 * Impact Components general I/O (Heat, Drought, Flood): Risk Contours (Geopackage, GeoJSON, OGC API Features).

- What other component(s) can interact with the component: Pixalytics Component: consume variables for Drought Indicator produced by Safe’s ARD component. Any other component that requires climate scenario summary ARD to drive DRI.

- What OGC standards or formats does the component use and produce: 
 * OGC API Features
 * Geopackage
 * NetCDF
 * GeoJSON
 * GeoTIFF/COG
 * As needed: GML, KML, PostGIS, geodatabase and about 400 other geospatial formats.

[[FMEARDworkflow]]
.High level FME ARD workflow showing generation of climate scenario ARD and impacts from climate model, EO, IoT, infrastructure and base map inputs
image::FME_ARD_workflow.PNG[FME_ARD_workflow]

==== Company Description 

Using the FME platform, Safe Software has been a leader in supporting geospatial interoperability for more than 25 years. A central goal has been to promote FAIR principles, including data sharing across barriers and silos, with unparalleled support for a wide array of both vendor specific formats and open standards. Safe Software also provides a range of tools to support interoperability workflows. FME Workbench is a graphical authoring environment that allows users to rapidly prototype transformation workflows in a no-code environment. FME Server then allows users to publish data transforms to enterprise oriented service architectures. FME Cloud offers a low cost, easy to deploy and scalable environment for deploying transformation and integration services to the cloud.

Open standards have always been a core strategy for Safe in order to support data sharing. SAIF (Spatial Archive Interchange Format) - the first format FME was built to support and the basis for the company name - was an open BC government standard that ultimately served as a basis for GML. We have supported open standards such as XML, JSON and OGC standards such as GML, KML, WMS, WFS for many years. 
Safe has collaborated over the years with the open standards community. For example, we have actively participated in the CityGML and INSPIRE communities in Europe. We have also been active within the OGC community and participated in many OGC initiatives including Maritime Limits and Boundaries, IndoorGML pilots and most recently the 2021 Disaster Pilot. Safe also actively participates in a number of Domain and Standards working groups including CityGML SWG, MUDDI SWG, 3DIM, EDM, Digital Twins, Health DWGs to name a few. 

==== Component Descriptions

D100 - Client instance: Analysis Ready Data Component

Our Analysis Ready Data component (ARD) uses the FME platform to consume regional climate model and EO data and generate FAIR datasets for downstream analysis and decision support. 

The challenge to manage and mitigate the effects of climate change poses difficulties for spatial and temporal data integration. One of the biggest gaps to date has been the challenge of translating the outputs of global climate models into specific impacts at the local level.  FME is ideally suited to help explore options for bridging this gap given its ability to read datasets produced by climate models such as NetCDF or OGC WCS. Then aggregate, interpolate and restructure it as needed, inter-relate it with higher resolution local data, and then output it to whatever format or service is most appropriate for a given application domain or user community.

Our ARD component supports the consumption of climate model outputs such as NetCDF, earth observation (EO) data, and the base map datasets necessary for downstream workflows including derivation of analysis ready datasets for impact analysis. It filters, interrelates and refines these datasets according to indicator requirements. After extraction, datasets are filtered by location and transformed to an appropriate resolution and CRS. Then the workflow classifies, resamples, simplifies and reprojects the data, and then defines feature IDs metadata and other properties to satisfy the target ARD requirements. This workflow is somewhat similar to what was needed to evaluate disaster impacts in DP21. Time ranges for climate scenarios are significantly longer - years rather than weeks for floods.

Once the climate model, and other supporting datasets have been adequately extracted, prepared and integrated, the final step is to generate the data streams and datasets required by downstream components and clients. The FME platform is well suited to deliver data in formats as needed. This includes Geopackage format for offline use. For online access, other open standards data streams are available, such as GeoJSON, KML or GML, via WFS and OGC Features APIs and other open APIs. 

As our understanding of end user requirements continues to evolve, this will necessitate changes in which data sources are selected and how they are refined, using a model based rapid prototyping approach. We anticipate that any operational system will need to support a growing range of climate change impacts and related domains. Tools and processes must be able to absorb and integrate new datasets into existing workflows with relative ease. As the pilot develops, data volumes will increase, requiring scalability methods to maintain performance and avoid overloading downstream components. Cloud based processing near cloud data sources using cloud native datasets (COG, STAC, etc) supports data scaling. Regarding the FME platform, this involves deployment of FME workflows to FME Cloud.

It is worth underlining that our ARD component depends on the appropriate data sources in order to produce the appropriate decision ready data (DRI) for downstream components. Risk factors include being able to locate and access suitable climate models and EO data of sufficient quality, resolution and timeliness to support indicators as the requirements and business rules associated with them evolve. Any data gaps encountered are documented under the lessons learned section. 


[[SafeSoftware_1]]
.Environment Canada NetCDF GCM  time series downscaled to Vancouver area. From: https://climate-change.canada.ca/climate-data/#/downscaled-data 
image::SafeSoftware_1.png[SafeSoftware_1]

[[SafeSoftware_2]]
.Data Cube to ARD: NetCDF to KML, Geopackage, GeoTIFF 
image::SafeSoftware_2.png[SafeSoftware_2]

Data workflow:
- Split data cube
- Set timestep parameters
- Compute timestep stats by band
- Compute time range stats by cell
- Classify by cell value range
- Convert grids to vector contour areas by class

[[SafeSoftware_3]]
.Extracted timestep  grids: Monthly timesteps, period mean T, period max T 
image::SafeSoftware_3.png[SafeSoftware_3]

[[SafeSoftware_4]]
.Convert raster temperature grids into temperature contour areas by class 
image::SafeSoftware_4.png[SafeSoftware_4]

[[SafeSoftware_5]]
.Geopackage Vector Area Time Series: Max Yearly Temp 
image::SafeSoftware_5.png[SafeSoftware_5]

==== D100 - Client Instance: Heat Impact Component

This component takes the climate scenario summary ARD results from the ARD component and analyzes them to derive estimated heat impacts over time, based on selected climate scenarios. Central to this is the identification of key heat impact indicators required by decision makers and the business rules needed to drive them. Process steps include data aggregation and statistical analysis of maximum temperature spikes, taking into account the cumulative impacts of multiple high temperature days. Data segmentation is based on maximum temperature exceeding a certain threshold T for N days in a row. This is because heat exhaustion effects are likely dependent on duration of heat spells, in addition to high maximum temperatures on certain days. 

[[SafeSoftware_6]]
.ARD Query: Monthly Max Temp Contours
image::SafeSoftware_6.png[SafeSoftware_6]

[[SafeSoftware_7]]
.ARD Query: Max Mean Monthly Temp > 25C 
image::SafeSoftware_7.png[SafeSoftware_7]

[[SafeSoftware_8]]
.Town of Lytton - location where entire town was devastated by fire during the heat wave of July 2021 - same location highlighted in ARD query from heat risk query in previous figure 
image::SafeSoftware_8.png[SafeSoftware_8]

==== D100 - Client Instance: Flood and Water Resource Impact Component

This component takes the climate scenario summary ARD results from the ARD component and analyzes them to derive estimated flood risk impacts over time, based on selected climate scenarios. Central to this will be the identification of key flood risk impact indicators required by decision makers and the business rules needed to drive them. This process includes data aggregation and statistical analysis of rainfall intensity over time, taking into account the cumulative impacts of multiple consecutive days. This involves, for example, data segmentation based on cumulative rainfall exceeding a certain threshold T within a certain time window (N hours or days), since cumulative rainfall and rainfall intensity over a short period are often more crucial than total rainfall over a longer period. These precipitation scenarios are evaluated by catch basin. This also requires integration with topography, DEMs, and hydrology related data such as river networks, water bodies. aquifers and watershed boundaries.

The FME transformation workflow classifies and segments the time series grid data, followed by vectorization and generalization in order to generate flood contour polygons by time step. The results are loaded to a geopackage which is more readily consumable by a wider variety of GIS applications and analytic tools. We have found that this vectorized data is relatively easy to publish to OGC API Feature Services.

[[SafeSoftware_9]]
.FME approach for converting flood time series grids to geopackage ARD 
image::SafeSoftware_9.png[SafeSoftware_9]

[[SafeSoftware_10]]
.Flood Contour Geopackage ARD, showing flooded areas south of Winnipeg by date and depth, as displayed in FME Data Inspector.
image::SafeSoftware_10.png[SafeSoftware_10]

==== D100 - Client Instance: Drought Impact Component

This component takes the climate scenario summary ARD results from the ARD component and analyze them to derive estimated drought risk impacts over time based on selected climate scenarios. This involves, for example, data segmentation based on cumulative rainfall below a certain threshold T within a certain time window (days, weeks or months), since cumulative rainfall over time will be crucial for computing water budgets by watershed or catch basin. Besides precipitation, climate models also generate soil moisture predictions which are used by this component to assess drought risk. This also requires integration with topography, DEMs and hydrology related data such as river networks, water bodies. aquifers and watershed boundaries. The specific business rules used to assess drought risk are still under development. FME provides a flexible data and business rule modeling framework. This means that as indicators and drought threshold rules are refined, it's relatively straightforward to adjust the business rules in this component to refine our risk projections. Also, business rule parameters can be externalized as execution parameters so that end users can control key aspects of the scenario drought risk assessment without having to modify the published FME workflow.

