
//[[clause-reference]]
== Raw data to Analysis Ready Data (ARD)

CEOS defines Analysis Ready Data as satellite data that have been processed to a minimum set of requirements and organized into a form that allows immediate analysis with a minimum of additional user effort and interoperability both through time and with other datasets. See https://ceos.org/ard/, and especially the infornation for data producers: https://ceos.org/ard/files/CARD4L_Info_Note_Producers_v1.0.pdf.



//[[CRIS]]
//.CRIS overview
//image::CRIS.png[CRIS]

//=== RSS-Hydro

Several past successful OGC testbeds, including the DP 21 to which this pilot is linked, have looked at ARD and IRD but also in terms of use cases. In this pilot, some main technical contributions have been creating digestible OGC data types and formats for specific partner use cases, so producing ARD from publically available EO and model data, including hydrological and other type of model output as well as climate projections.

These ARD will feed into all use cases for all participants, with a particular focus toward the use cases proposed for Heat, Drought and Health Impacts by participants in the pilot. 

Specifically, participants, like RSS-Hydro, provide access to the following satellite and climate projection data:
 * Wildfire – Fire Radiant Power (FRP) product from Sentinel 3 (NetCDF), 5p, MODIS products (fire detection), VIIRS (NOAA); possibly biomass availability (fire fuel).
 * Land Surface Temp - Sentinel 3
 * Pollution - Sentinel 5p
 * Climate Projection data (NetCDF, etc., daily downscaled possible): air temp (10 m above ground). Rainfall and possibly wind direction as well
 * Satellite-derived Discharge Data to look at Droughts/Floods etc. by basin or other scale
 * Can provide some hydrological model simulation outputs at (sub)basin scale (within reason)

The created ARD in various OGC interoperable formats created digestible dataflows for the proposed OGC Use Cases. This proposed data chain by several participants is similar to DP21. A generated climate indicator or EO remotely sensed data (NASA, NOAA, ESA,  etc.) from various sources are "simplified"to GeoTIFF and / or vectorized geopackage per time step by other participants' tools, such as the FME software (by SafeSoftware). Another option as an intermediate data type (IRD) can be COG - cloud optimized geotiff which would make access more efficient. The COG GeoTIFFs are optimized for cloud so data sharing can be made more efficient. ARD and IRD should become more service / cloud based wherever possible.

Besides the data format, data structures and semantics required to support the desired DRI’s are important. The time series / raster, and classification to vector contour transform is an approach that worked well in DP21 and has been a good starting point also in this pilot. For example, together in the FME processing engine, time series grids can be aggregated across timesteps to mean or max values, then classify them into ranges suitable for decision making, and then write them out and expose them as time tagged vector contour tables.

In summary, the different ARD and IRD data can be created from the following data sources:
 * Inputs: EO (US sources fire related: MODIS, VIIRS); Climate projections, sub catchment polygons, assisting Albert with EO Europe sources; Sentinel-3, Sentinel 5-P
 * Outputs forma & instances: WCS, GeoTIFF spatial / temporal subset, Shape; NetCDF
 * Output parameters: e.g. hydrological condition of a basin (historically/current). So drought / flood etc.
 * Output themes: downscaled / subset outputs, hydrologic scenarios


//=== GMU_CSISS

Other Inputs: ECV record information, OpenSearch service endpoint (currently CMR(CWIC) and FedEO), download URLs for accessing NetCDF or HDF files.

- Outputs: WCS service endpoint for accessing selected granule level product images (GeoTIFF, PNG, JPEG, etc.).

- What other component(s) can interact with the component: .

- What OGC standards or formats does the component use and produce: 
  * WCS for downloading image
  * WMS for showing layers on basemap



=== Pixalytics

Pixalytics have developed an OGC-compliant Application Programming Interface (API) service, see <<Pixalytics_architecture>>, which will provide global information on droughts. The approach is to take global open data/datasets from organizations such as ESA/Copernicus, NASA/NOAA, and the WMO and combine meteorology, hydrology, and remote sensing data to produce ARD data based on a composite of different indicators. Where globally calculated drought indicators already exist, these are being used in preference to their re-calculation, although consistency and the presence of uncertainties are also being considered.

[[Pixalytics_architecture]]
.Pixalytics drought severity workflow architecture
image::Pixalytics-architecture.png[Pixalytics-architecture]

The Drought Severity Workflow (DSW) is built using individual drought indicators for precipitation (SPI), soil moisture (SMA), and vegetation drought that are together using the Combined Drought Indicator (CDI) as described by [<<cite_Sepulcre-Canto_2012>>]. The API access has been set up following the Building Blocks for Climate Services (https://climateintelligence.github.io/smartduck-docs/) approach.

- Component: D100 Drought indicator.

- Inputs: Meteorological data, including Precipitation, plus Land Surface Temperature, Soil Moisture, and Vegetation Index (or optical data to calculate it from).

- Outputs: Drought Indices - default is CDI - as a time-series dataset output in a choice of download able formats: CSV, GeoJSON (default), CoverageJSON and NetCDF for point data and then COG for areas (to be developed).

- What other component(s) can interact with the component: a desire to link to visualization/DRI analysis components. A QGIS plugin has been updated to be able to perform a request and view the outputted JSON file (https://github.com/pixalytics-ltd/qgis-wps-plugin), and the Web Processing Service (WPS) link is https://api.pixalytics.com/climate/wps?request=GetCapabilities&service=wps

An example Python query for a location in Canada (Latitude: 55.5 N Longitude: 99.1 W) for the SPI time series, with data for these dates/this location already cached, so runs quicker:

.Drought indicator calling code that generated the default output, which is the CDI in GeoJSON format
----
    from owslib.wps import WebProcessingService, monitorExecution
    
	# contact the WPS client
    wps = WebProcessingService("http://api.pixalytics.com/climate/wps", skip_caps=True)
    
    # GetCapabilities
    wps.getcapabilities()

	# Execute
    inputs = [ ("start_date", '20200101'),
            ("end_date", '20221231'),
            ("latitude", '55.5'),
            ("longitude", '-99.1')]
    
    execution = wps.execute("drought", inputs, "output")

    outfile = "temp.json"
    monitorExecution(execution,download=True,filepath=outfile)

    # Wait 5 seconds and check
    execution.checkStatus(sleepSecs=5)

    # show status
    print('Percent complete {}%, generated {}'.format(execution.percentCompleted, outfile))

	# If there's an error print the error information
    for error in execution.errors:
        print("Error: ",error.code, error.locator, error.text)
----

- What OGC standards or formats does the component use and produce: Producing data on-the-fly using the WPS, so need to pull data through preferably an API route. The speed that the input data can be made available (i.e., extracting time-series subsets) governs the speed that the drought indicator provides data. To speed this up, input data that is not changing is being cached so that it runs significantly quicker when the API is called for a second time. 

<<Pixalytics_output>> shows an example of the output visualized within Python using Streamlit with the intermediate data (cached as NetCDF files) as input.

[[Pixalytics_output]]
.Pixalytics output of the CDI for a point location in Canada (Latitude: 55.5 N Longitude: 99.1 W); generated using Copernicus Emergency Management Service information [2023]
image::Pixalytics-output-example.png[Pixalytics-output]


==== Data Sources

_The Global Drought Observatory_

The Global Drought Observatory (GDO), owned by the Copernicus Emergency Management Services, provides a global map of coarsely-gridded agricultural drought risk, along with a breakdown of the risk for each country. The drought risk is computed using the CDI, with the variables used to compute it and other drought-related variables provided in the user portal for https://edo.jrc.ec.europa.eu/gdo/php/index.php?id=2112[download], but the CDI itself is not available for download and so is being calculated in the DSW.

[[GDO-screenshot]]
.Global Drought Observatory Web Portal, https://edo.jrc.ec.europa.eu/gdo/php/index.php?id=2001
image::GDO_screenshot.png[GDO-screenshot]

We obtain SMA and Fraction of Absorbed Photosynthetically Active Radiation (FAPAR) from the GDO data download service. These are provided as netCDF files and contain pre-computed anomalies, so can be assimilated directly into the back-end. The SMA uses a combination of the root soil moisture from the https://ec-jrc.github.io/lisflood-model/[LISFLOOD model], the MODIS land surface temperature and the ESA Climate Change Initiative (CCI) skin soil moisture [<<cite_Cammalleri_2016>>], and the FAPAR is from NASA optical imagery.

_ERA5 Reanalysis from ECMWF CDS_

The CDS portal provides an API interface to return either hourly or monthly averages of the ERA5 variables. Requesting the hourly data is necessary to compute anything which requires a frequency greater than monthly, which is the case for most drought indicators (e.g. SMA) which are in dekads. To ensure there is no anti-aliasing, the full 24hr dataset for each day of the month must be downloaded. This is very time-consuming and requests will fail if the number of data points exceeds the limit, which will occur for a period of 2 years or more, even for a single location.

There is a separate application, which can also be accessed via API, to return daily data. The CDS employs a queue management system, which determines the priority of each request based partially on the computational demand of the request. The daily data retrieval relies upon an underlying service to compute the daily statistics from the hourly data, demanding more resources than simply extracting the hourly or monthly data which are pre-computed. This means the request is held in the queue for a long time (up to hours), so there is no time benefit over using the hourly data. However, for a longer time-period which would be rejected if requested hourly, this provides a workaround. A further benefit of requesting daily, rather than hourly, data is that the downloaded file is smaller.

We compute SPI and SMA using variables from the CDS API. The SPI is computed from the total precipitation in monthly intervals. The SMA is computed from the soil water volume, which is available for 4 depth levels. The SMA for each depth is computed by calculating the z-score against a long term mean, using the same baseline time period as the SPI. The most relevant depth layer can then be selected by the user; for instance, a user interested in the health of crops with shallow roots may wish to access the surfacemost layer.

_ERA5 Reanalysis from AWS_

Input precipitation data was also tested using https://registry.opendata.aws/ecmwf-era5/[ERA5 data held within the Registry of Open Data on AWS] versus the CDS API and found the Amazon Web Service (AWS) Simple Storage Service (S3) stored data could be accessed faster once virtual Zarrs has been setup, but there is a question over provenance as the data on AWS was put there by an organization other than the data originator and the Zarr approach didn't work for more recent years as the S3 stored NetCDFs have chunking that is inconsistent. An issue was raised for the Python kerchunck library, to be able to cope with variable chunking, as this https://github.com/zarr-developers/zeps/blob/main/draft/ZEP0003.md[isn't current supported].

_NOAA API_

The NOAA API is OGC-compliant and easy to access using OGC-style queries, however is still at an early stage of development and only runs from 9am to 5pm EST, Monday to Friday. Several sources of precipitation data are provided including grided observational data from NOAA's https://www.drought.gov/data-maps-tools/global-historical-climatology-network-ghcn[Global Historical Climatology Network] https://www.drought.gov/data-maps-tools/gridded-climate-datasets-noaas-nclimgrid-monthly[(nClimGrid)] and CMIP data from the https://www.nasa.gov/nex/gddp[NASA-GDDP] and https://loca.ucsd.edu/[LOCA2] downscaling projects. These datasets are only available for continental North America.
We use the precipitation parameter from nClimGrid to compute a monthly SPI with data from 1985 to the present day. This can also be incorporated into the CDI. Further work could include using the LOCA2 projections to predict the SPI in future months/years.

_Safe Software_

We reviewed the GeoJSON point data extracted from the https://climate-change.canada.ca/climate-data/#/downscaled-data[Climate scenario RPC4.5 downscaled for Canada] provided by Safe Software. As the provided point dataset contains precipitation anomaly data (percentage change compared to a reference period), then it isn't a straightforward input for calculating SPI but can be visualized alongside it as another view of the meteorological drought conditions. 

==== Further work

The work in this Pilot has focused on building this initial version of the workflow, deploying it via WPS and pulling data from different sources to understand the advantages and disadvantages of the different sources, including straightforwardness and speed of accessibility. For future Pilot activities we plan to continue to build the robustness of the approach, including testing and improving on the robustness of the interfaces to the input data sources and output provided to other Pilot components.

The current work has focused on the extraction and generation of a point time-series, and so there are plans to expand the code to the extraction and generation of a 3D data cube. This might involve changing the output API interface to the OGC Environmental Data Retrieval (EDR) API standard.

=== Safe Software

- Component:
 * Climate ARD component - Data Cube to ARD.
 * Impact Components general I/O (Heat, Drought, Flood).

- Inputs: 
 * Climate ARD component - Data Cube to ARD: Climate scenario data from climate services (NetCDF), for historic and future time periods
 * Impact Components general I/O (Heat, Drought, Flood): Climate impact ARD from Safes ARD component, including EO data (MODIS, LANDSAT, SENTINEL products), Population/Infrastructure information (OSM), Basemaps, as well as specific requirements per impact:
  * Drought: vegetation, soils, hydrology, basins
  * Flood: DEM, hydrology, basins.

- Outputs:
 * Climate ARD component - Data Cube to ARD: Gridded data, including temperature, soil moisture and  precipitation - aggregate grids (GeoTIFF/COG), as well as Vector data, including temperature, soil moisture and  precipitation contours (Geopackage, GeoJSON, OGC API Features).
 * Impact Components general I/O (Heat, Drought, Flood): Risk Contours (Geopackage, GeoJSON, OGC API Features).

- What other component(s) can interact with the component: Pixalytics Component: consume variables for Drought Indicator produced by Safe’s ARD component. Any other component that requires climate scenario summary ARD to drive DRI.

- What OGC standards or formats does the component use and produce: 
 * OGC API Features
 * Geopackage
 * NetCDF
 * GeoJSON
 * GeoTIFF/COG
 * As needed: GML, KML, PostGIS, geodatabase and about 400 other geospatial formats.

[[FMEARDworkflow]]
.High level FME ARD workflow showing generation of climate scenario ARD and impacts from climate model, EO, IoT, infrastructure and base map inputs
image::FME_ARD_workflow.PNG[FME_ARD_workflow]

==== Company Description 

Using the FME platform, Safe Software has been a leader in supporting geospatial interoperability for more than 25 years. A central goal has been to promote FAIR principles, including data sharing across barriers and silos, with unparalleled support for a wide array of both vendor specific formats and open standards. Safe Software also provides a range of tools to support interoperability workflows. FME Workbench is a graphical authoring environment that allows users to rapidly prototype transformation workflows in a no-code environment. FME Server then allows users to publish data transforms to enterprise oriented service architectures. FME Cloud offers a low cost, easy to deploy and scalable environment for deploying transformation and integration services to the cloud.

Open standards have always been a core strategy for Safe in order to support data sharing. SAIF (Spatial Archive Interchange Format) - the first format FME was built to support and the basis for the company name - was an open BC government standard that ultimately served as a basis for GML. We have supported open standards such as XML, JSON and OGC standards such as GML, KML, WMS, WFS for many years. 
Safe has collaborated over the years with the open standards community. For example, we have actively participated in the CityGML and INSPIRE communities in Europe. We have also been active within the OGC community and participated in many OGC initiatives including Maritime Limits and Boundaries, IndoorGML pilots and most recently the 2021 Disaster Pilot. Safe also actively participates in a number of Domain and Standards working groups including CityGML SWG, MUDDI SWG, 3DIM, EDM, Digital Twins, Health DWGs to name a few. 

==== Component Descriptions

D100 - Client instance: Analysis Ready Data Component

Our Analysis Ready Data component (ARD) uses the FME platform to consume regional climate model and EO data and generate FAIR datasets for downstream analysis and decision support. 

The challenge to manage and mitigate the effects of climate change poses difficulties for spatial and temporal data integration. One of the biggest gaps to date has been the challenge of translating the outputs of global climate models into specific impacts at the local level.  FME is ideally suited to help explore options for bridging this gap given its ability to read datasets produced by climate models such as NetCDF or OGC WCS. Then aggregate, interpolate and restructure it as needed, inter-relate it with higher resolution local data, and then output it to whatever format or service is most appropriate for a given application domain or user community.

Our ARD component supports the consumption of climate model outputs such as NetCDF, earth observation (EO) data, and the base map datasets necessary for downstream workflows including derivation of analysis ready datasets for impact analysis. It filters, interrelates and refines these datasets according to indicator requirements. After extraction, datasets are filtered by location and transformed to an appropriate resolution and CRS. Then the workflow classifies, resamples, simplifies and reprojects the data, and then defines feature IDs metadata and other properties to satisfy the target ARD requirements. This workflow is somewhat similar to what was needed to evaluate disaster impacts in DP21. Time ranges for climate scenarios are significantly longer - years rather than weeks for floods.

Once the climate model, and other supporting datasets have been adequately extracted, prepared and integrated, the final step is to generate the data streams and datasets required by downstream components and clients. The FME platform is well suited to deliver data in formats as needed. This includes Geopackage format for offline use. For online access, other open standards data streams are available, such as GeoJSON, KML or GML, via WFS and OGC Features APIs and other open APIs. 

As our understanding of end user requirements continues to evolve, this will necessitate changes in which data sources are selected and how they are refined, using a model based rapid prototyping approach. We anticipate that any operational system will need to support a growing range of climate change impacts and related domains. Tools and processes must be able to absorb and integrate new datasets into existing workflows with relative ease. As the pilot develops, data volumes will increase, requiring scalability methods to maintain performance and avoid overloading downstream components. Cloud based processing near cloud data sources using cloud native datasets (COG, STAC, etc) supports data scaling. Regarding the FME platform, this involves deployment of FME workflows to FME Cloud.

It is worth underlining that our ARD component depends on the appropriate data sources in order to produce the appropriate decision ready data (DRI) for downstream components. Risk factors include being able to locate and access suitable climate models and EO data of sufficient quality, resolution and timeliness to support indicators as the requirements and business rules associated with them evolve. Any data gaps encountered are documented under the lessons learned section. 


[[SafeSoftware_1]]
.Environment Canada NetCDF GCM  time series downscaled to Vancouver area. From: https://climate-change.canada.ca/climate-data/#/downscaled-data 
image::SafeSoftware_1.png[SafeSoftware_1]

[[SafeSoftware_2]]
.Data Cube to ARD: NetCDF to KML, Geopackage, GeoTIFF 
image::SafeSoftware_2.png[SafeSoftware_2]

Data workflow:
- Split data cube
- Set timestep parameters
- Compute timestep stats by band
- Compute time range stats by cell
- Classify by cell value range
- Convert grids to vector contour areas by class

[[SafeSoftware_3]]
.Extracted timestep  grids: Monthly timesteps, period mean T, period max T 
image::SafeSoftware_3.png[SafeSoftware_3]

[[SafeSoftware_4]]
.Convert raster temperature grids into temperature contour areas by class 
image::SafeSoftware_4.png[SafeSoftware_4]

[[SafeSoftware_5]]
.Geopackage Vector Area Time Series: Max Yearly Temp 
image::SafeSoftware_5.png[SafeSoftware_5]

==== D100 - Client Instance: Heat Impact Component

This component takes the climate scenario summary ARD results from the ARD component and analyzes them to derive estimated heat impacts over time, based on selected climate scenarios. Central to this is the identification of key heat impact indicators required by decision makers and the business rules needed to drive them. Process steps include data aggregation and statistical analysis of maximum temperature spikes, taking into account the cumulative impacts of multiple high temperature days. Data segmentation is based on maximum temperature exceeding a certain threshold T for N days in a row. This is because heat exhaustion effects are likely dependent on duration of heat spells, in addition to high maximum temperatures on certain days. 

[[SafeSoftware_6]]
.ARD Query: Monthly Max Temp Contours
image::SafeSoftware_6.png[SafeSoftware_6]

[[SafeSoftware_7]]
.ARD Query: Max Mean Monthly Temp > 25C 
image::SafeSoftware_7.png[SafeSoftware_7]

[[SafeSoftware_8]]
.Town of Lytton - location where entire town was devastated by fire during the heat wave of July 2021 - same location highlighted in ARD query from heat risk query in previous figure 
image::SafeSoftware_8.png[SafeSoftware_8]

==== D100 - Client Instance: Flood and Water Resource Impact Component

This component takes the climate scenario summary ARD results from the ARD component and analyzes them to derive estimated flood risk impacts over time, based on selected climate scenarios. Central to this will be the identification of key flood risk impact indicators required by decision makers and the business rules needed to drive them. This process includes data aggregation and statistical analysis of rainfall intensity over time, taking into account the cumulative impacts of multiple consecutive days. This involves, for example, data segmentation based on cumulative rainfall exceeding a certain threshold T within a certain time window (N hours or days), since cumulative rainfall and rainfall intensity over a short period are often more crucial than total rainfall over a longer period. These precipitation scenarios are evaluated by catch basin. This also requires integration with topography, DEMs, and hydrology related data such as river networks, water bodies. aquifers and watershed boundaries.

The FME transformation workflow classifies and segments the time series grid data, followed by vectorization and generalization in order to generate flood contour polygons by time step. The results are loaded to a geopackage which is more readily consumable by a wider variety of GIS applications and analytic tools. We have found that this vectorized data is relatively easy to publish to OGC API Feature Services.

[[SafeSoftware_9]]
.FME approach for converting flood time series grids to geopackage ARD 
image::SafeSoftware_9.png[SafeSoftware_9]

[[SafeSoftware_10]]
.Flood Contour Geopackage ARD, showing flooded areas south of Winnipeg by date and depth, as displayed in FME Data Inspector.
image::SafeSoftware_10.png[SafeSoftware_10]

==== D100 - Client Instance: Drought Impact Component

This component takes the climate scenario summary ARD results from the ARD component and analyze them to derive estimated drought risk impacts over time based on selected climate scenarios. This involves, for example, data segmentation based on cumulative rainfall below a certain threshold T within a certain time window (days, weeks or months), since cumulative rainfall over time will be crucial for computing water budgets by watershed or catch basin. Besides precipitation, climate models also generate soil moisture predictions which are used by this component to assess drought risk. This also requires integration with topography, DEMs and hydrology related data such as river networks, water bodies. aquifers and watershed boundaries. The specific business rules used to assess drought risk are still under development. FME provides a flexible data and business rule modeling framework. This means that as indicators and drought threshold rules are refined, it's relatively straightforward to adjust the business rules in this component to refine our risk projections. Also, business rule parameters can be externalized as execution parameters so that end users can control key aspects of the scenario drought risk assessment without having to modify the published FME workflow.


=== Wuhan University (WHU)-Component

Wuhan University (WHU) is a university that plays a significant role in researching and teaching all aspects of surveying and mapping, remote sensing, photogrammetry, and geospatial information sciences in China. In this Climate Resilience Pilot, WHU will contribute three components (ARD, Drought Indicator, and Data Cube) and one use-case (Drought Impact Use-cases). 

==== Component: ARD 

- Inputs: Gaofen L1A data and Sentinel-2 L1C data
- Outputs: Surface Reflectance ARD
- What other component(s) can interact with the component: Any components requiring access to surface reflectance data

Surface Reflectance (SR) is the fraction of incoming solar radiation reflected from the Earth's surface for specific incidents or viewing cases. It can be used to detect the distribution and change of ground objects by leveraging the derived spectral, geometric, and textural features. Since a large amount of optical EO data has been released to the public, ARD can facilitate interoperability through time and multi-source datasets. As the probably most widely applied ARD product type, the SR ARD can contribute to climate resilience research. For example, the SR-derived NDVI series can be applied to monitor wildfire recovery by analyzing vegetation index increases. Several SR datasets have been assessed as ARD by CEOS, like the prestigious Landsat Collection 2 Level 2, and Sentinel-2 L2A, while many other datasets are still provided at a low processing level.

WHU is developing a pre-processing framework for SR ARD generation. The framework supports radiometric calibration, geometric ratification, atmospheric correction, and cloud mask. To address the inconsistencies in observations from different platforms, including variations in band settings and viewing angles, we proposed a processing chain to produce harmonized ARD. This will enable us to generate SR ARD with consistent radiometric and geometric characteristics from multi-sensor data, resulting in improved temporal coverage. In the first stage of our mission, we are focusing on the harmonization of Chinese Gaofen data and Sentinel-2 data, as shown in Figure 1, the harmonization involves spatial co-registration, band conversion, and bidirectional reflectance distribution function (BRDF) correction. Figure 2 shows the Sentinel-2 data before and after pre-processing. Furthermore, we wish to seek the assessment of CEOS-ARD in our long-term plan.

[[WHU_image1]]
.The processing chain to produce harmonized ARD.
image::WHU_image1.png[WHU_image1]

[[WHU_image2]]
.Sentinel-2 RBG composite (red Band4, green Band3, blue Band2), over Hubei, acquired on October 22, 2020. (a) corresponds to the reflectance at the top of the atmosphere (L1C product); (b) corresponds to the surface reflectance after pre-processing.
image::WHU_image2.png[WHU_image2]


==== Component: Drought Indicator 
- Inputs: Climate data, including precipitation and temperature
- Outputs: Drought risk map derived from drought indicator
- What other component(s) can interact with the component: Any components requiring access to drought risk map through OGC API
- What OGC standards or formats does the component use and produce: OGC API - Processes

Drought is a disaster whose onset, end, and extent are difficult to detect. Original meteorological data, such as precipitation, can be obtained through satellites and radar, which can be used for drought monitoring. However, the accuracy is easily affected by detection instruments and terrain occlusion, and the ability to retrieve special shapes, such as solid precipitation, is limited. In addition, many meteorological monitoring stations on the ground can provide local raw meteorological observation data. The SPEI is a model to monitor, quantitatively analyze, and determine the spatiotemporal range of the occurrence of drought using meteorological observation data from various regions. It should supplement the result of drought monitoring with satellite and radar.

SPEI has two main characteristics: 1) it considers the deficits between precipitation and evapotranspiration comprehensively, that is, the balance of water; 2) multi-time scale characteristics. For 1) drought is caused by insufficient water resources. Precipitation can increase water, while evapotranspiration can reduce water. The differences between the two variables simultaneously and in space can characterize the balance of water. For 2), the deficits value of different usable water sources is distinct at different time scales due to the different evolution cycles of different types, resulting in various representations in temporal. By accumulating the difference between precipitation and evapotranspiration at different time scales, agricultural (soil moisture) droughts, hydrological (groundwater, streamflow, and reservoir) droughts, and other droughts can be distinguished by SPEI.

In our project, the dataset for SPEI calculation is ERA5-Land monthly averaged data from 1950 to the present. We selected years of data about partial areas of East Asia for experiments. Through the following flow of the SPEI calculation, we obtain the SPEI value for assessments of drought impact. The flow of the SPEI calculation is shown in Figure 3.

[[WHU_image3]]
.Flow of the SPEI calculation.
image::WHU_image3.png[WHU_image3]

WHU has provided the SPEI drought index calculation services through the OGC API - Processes, enabling interaction with other components. The current endpoint for OGC API - Processes is http://oge.whu.edu.cn/ogcapi/processes_api. This section will explain how to use this API for calculating the drought index.

- Example：/processes
http://oge.whu.edu.cn/ogcapi/processes_api/processes
The API endpoint for retrieving the processes list.
- Example：/processes/{processId}
http://oge.whu.edu.cn/ogcapi/processes_api/processes/spei
The API endpoint for retrieving a process description (e.g. spei). This returns the description of "spei" process, which contains the inputs and outputs information.
- Example：/processes/{processId}/execution
http://oge.whu.edu.cn/ogcapi/processes_api/processes/spei/execution
The API endpoint for executing the process. The spei process exclusively supports asynchronous execution, resulting in the creation of a job for processing.
The request body:

{
	"inputs": {
		"startTime": "2010-01-01",
		"endTime": "2020-01-01",
       "timeScale": 5,
		"extent": {
			"bbox": [73.95, 17.95, 135.05, 54.05],
			"crs": "http://www.opengis.net/def/crs/OGC/1.3/CRS84"
		}
	}
}

- Example：/processes/{processId}/jobs/{jobId}
http://oge.whu.edu.cn/ogcapi/processes_api/processes/spei/jobs/{jobId}
The API endpoint for retrieving status of a job.
- Example：/processes/{processId}/jobs/{jobId}/results
http://oge.whu.edu.cn/ogcapi/processes_api/processes/spei/jobs/{jobId}/results
The API endpoint for retrieving the results of a job, which are encoded as :
[{
		"value": {
			"time": "2000_02_01",
			"url": "http://oge.whu.edu.cn/api/oge-python/data/temp/9BC500C1B0E3438C090AF5C6F8602045/8d0357fb-8ffb-4e62-9c3a-55ad17a5831a/SPEI_2000_02_01.png"
		}
	},
	......
]

[[WHU_image4]]
.The SPEI results for the date 2000_02_01.
image::WHU_image4.png[WHU_image4]

==== Component: Data Cube
- Inputs: ERA5 temperature and precipitation data
- Outputs: Results in the form of GeoTIFF after processing in Data Cubes
- What other component(s) can interact with the component: Any components requiring access to temperature and precipitation data in part of Asia through OGC API
- What OGC standards or formats does the component use and produce: OGC API- Coverages

WHU has introduced GeoCube as a cube infrastructure for the management and large-scale analysis of multi-source data. GeoCube leverages the latest generation of OGC standard service interfaces, including OGC API-Coverages, OGC API-Features, and OGC API-Processes, to offer services encompassing data discovery, access, and processing of diverse data sources. The UML model of the GeoCube is given in Figure 5, and it has four dimensions: product, spatial, temporal, and band. Product dimension specifies the thematic axis for the geospatial data cube using the product name (e.g. ERA5_Precipitation or OSM_Water), type (e.g. raster, vector, or tabular), processes, and instrument name. For example, the product dimension can describe optical image products by recording information on the instrument and band. Spatial dimension specifies the spatial axis for the geospatial data cube using the grid code, grid type, city name, and province name. The cube uses a spatial grid for tiling to enable data readiness in a high-performance form. Temporal dimension specifies the temporal axis for the geospatial data using the phenomenon time and result time. Band dimension describes the band attribute of the raster products according to the band name, polarization mode that is reserved for SAR images, and product-level band. The product-level band is the information that is extracted from the original bands. For example, the Standardized Precipitation Evapotranspiration Index (SPEI) band is a product-level band that takes into account the hydrological process and evaluates the degree of drought by calculating the balance of precipitation and evaporation.

[[WHU_image5]]
.The UML model of WHU Data Cube.
image::WHU_image5.png[WHU_image5]


WHU has organized ERA5 temperature and precipitation data into a cube and offers climate data services through the OGC API - Coverages, supporting the computation of various climate indices. The API endpoint is http://oge.whu.edu.cn/ogcapi/coverages_api, allowing users to query and retrieve the desired data from the cube. This section provides examples demonstrating how to access the data from the cube using OGC API - Coverages.

- Example：/collections
http://oge.whu.edu.cn/ogcapi/coverages_api/collections?bbox=112.65942,29.23223,115.06959,31.36234&limit=10&time=2016-01-01T02:55:50Z/2018-01-01T02:55:50Z
The API endpoint for querying datasets from the cube, and the query parameters including limit, bbox, and time.
- Example：/collections/{collectionId}
http://oge.whu.edu.cn/ogcapi/coverages_api/collections/2m_temperature_201602
The API endpoint for retrieving the description of the coverage with the specified ID from the cube. 
- Example：/collections/{collectionId}/coverage
http://oge.whu.edu.cn/ogcapi/coverages_api/collections/2m_temperature_201602/coverage
The API endpoint for retrieving the coverage in GeoTIFF format for the specified ID. Here is an example of the response:

[[WHU_image6]]
.The coverage with the ID "2m_temperature_201602" in the Asian region.
image::WHU_image6.png[WHU_image6]

- Example：/collections/{collectionId}/coverage/rangetype
http://oge.whu.edu.cn/ogcapi/coverages_api/collections/2m_temperature_201602/coverage/rangetype
The API endpoint for accessing the range type of the coverage, which is part of the band dimension members in the cube. In this example, the coverage consists of only one band dimension member.
- Example：/collections/{collectionId}/coverage/domainset
http://oge.whu.edu.cn/ogcapi/coverages_api/collections/2m_temperature_201602/coverage/domainset 
The API endpoint for the domain set of the coverage, which is also the domain set of the cube.

